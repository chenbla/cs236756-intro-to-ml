{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img src=\"https://img.icons8.com/dusk/64/000000/mind-map.png\" style=\"height:50px;display:inline\"> CS 236756 - Technion - Intro to Machine Learning\n",
    "#### Tal Daniel\n",
    "## Tutorial 10 - Boosting & Bagging\n",
    "\n",
    "\n",
    "### <img src=\"https://img.icons8.com/bubbles/50/000000/checklist.png\" style=\"height:50px;display:inline\"> Agenda\n",
    "\n",
    "* Ensemble Learning\n",
    "    * Voting Classifiers\n",
    "* Bagging\n",
    "    * Boostrap\n",
    "* Boosting\n",
    "    * AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <img src=\"https://img.icons8.com/dusk/64/000000/elections.png\" style=\"height:50px;display:inline\"> Ensemble Learning\n",
    "* **Wisdom of the Crowd** - assembling the predictions of a group of predictors (such as classifiers or regressors) often results in a better prediction than with the best individual predictor.\n",
    "* **Ensemble** - a group of predictors. An *Ensemble Learning* algorithm is called an **Ensemble method**.\n",
    "    * For example: **Random Forest** -train a group of Decision Tree classifiers, each is trained on a random subset of the training set. To make predicitons, we obtain the predicitons of all individual trees, and then predict the class that gets the most votes. This is one of the most powerful ML algorithms available today.\n",
    "\n",
    "### <img src=\"https://img.icons8.com/flat_round/64/000000/vote-button.png\" style=\"height:50px;display:inline\"> Voting Classifiers\n",
    "* **Hard Voting Classifier** - aggregate the predictions of each classifier and predict the class that gets the most votes.\n",
    "    * In fact, even if each classifier is a *weak learner* (it does only slightly better than random guessing), the ensemble can still be a *strong learner* (achieving high accuracy), provided there are a sufficient number of weak learners and they are sufficiently diverse.\n",
    "    * **The Law of Large Numbers** - how can the above fact be explained? building an ensemble containing 1,000 classifiers that are individually correct only 51% of the time (slighly better than random guessing) and predict the majority voted class, it is possible to reach 75% accuracy if all the classifiers are perfectly independent (which is not really the case since they are trained on the same data).\n",
    "    * One way to get diverse classifiers is to train them using very different algorithms (increases the chance that they will make very different types of erros and thus improving the ensemble's accuracy).\n",
    "* **Soft Voting Classifier** - if all the classifiers are able to estimate class probabilities, then the class probability can be averaged over all the individual classifiers.\n",
    "    * It often achieves higher performance than *hard voting* because it gives more weight to highly confident votes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for the tutorial\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total samples: 569\n",
      "total positive sampels (M): 212, total negative samples (B): 357\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "      <th>Unnamed: 32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>527</th>\n",
       "      <td>91813702</td>\n",
       "      <td>B</td>\n",
       "      <td>12.340</td>\n",
       "      <td>12.27</td>\n",
       "      <td>78.94</td>\n",
       "      <td>468.5</td>\n",
       "      <td>0.09003</td>\n",
       "      <td>0.06307</td>\n",
       "      <td>0.02958</td>\n",
       "      <td>0.02647</td>\n",
       "      <td>...</td>\n",
       "      <td>19.27</td>\n",
       "      <td>87.22</td>\n",
       "      <td>564.9</td>\n",
       "      <td>0.1292</td>\n",
       "      <td>0.2074</td>\n",
       "      <td>0.17910</td>\n",
       "      <td>0.10700</td>\n",
       "      <td>0.3110</td>\n",
       "      <td>0.07592</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524</th>\n",
       "      <td>917897</td>\n",
       "      <td>B</td>\n",
       "      <td>9.847</td>\n",
       "      <td>15.68</td>\n",
       "      <td>63.00</td>\n",
       "      <td>293.2</td>\n",
       "      <td>0.09492</td>\n",
       "      <td>0.08419</td>\n",
       "      <td>0.02330</td>\n",
       "      <td>0.02416</td>\n",
       "      <td>...</td>\n",
       "      <td>22.99</td>\n",
       "      <td>74.32</td>\n",
       "      <td>376.5</td>\n",
       "      <td>0.1419</td>\n",
       "      <td>0.2243</td>\n",
       "      <td>0.08434</td>\n",
       "      <td>0.06528</td>\n",
       "      <td>0.2502</td>\n",
       "      <td>0.09209</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>906878</td>\n",
       "      <td>B</td>\n",
       "      <td>13.660</td>\n",
       "      <td>19.13</td>\n",
       "      <td>89.46</td>\n",
       "      <td>575.3</td>\n",
       "      <td>0.09057</td>\n",
       "      <td>0.11470</td>\n",
       "      <td>0.09657</td>\n",
       "      <td>0.04812</td>\n",
       "      <td>...</td>\n",
       "      <td>25.50</td>\n",
       "      <td>101.40</td>\n",
       "      <td>708.8</td>\n",
       "      <td>0.1147</td>\n",
       "      <td>0.3167</td>\n",
       "      <td>0.36600</td>\n",
       "      <td>0.14070</td>\n",
       "      <td>0.2744</td>\n",
       "      <td>0.08839</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>844359</td>\n",
       "      <td>M</td>\n",
       "      <td>18.250</td>\n",
       "      <td>19.98</td>\n",
       "      <td>119.60</td>\n",
       "      <td>1040.0</td>\n",
       "      <td>0.09463</td>\n",
       "      <td>0.10900</td>\n",
       "      <td>0.11270</td>\n",
       "      <td>0.07400</td>\n",
       "      <td>...</td>\n",
       "      <td>27.66</td>\n",
       "      <td>153.20</td>\n",
       "      <td>1606.0</td>\n",
       "      <td>0.1442</td>\n",
       "      <td>0.2576</td>\n",
       "      <td>0.37840</td>\n",
       "      <td>0.19320</td>\n",
       "      <td>0.3063</td>\n",
       "      <td>0.08368</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>906024</td>\n",
       "      <td>B</td>\n",
       "      <td>12.700</td>\n",
       "      <td>12.17</td>\n",
       "      <td>80.88</td>\n",
       "      <td>495.0</td>\n",
       "      <td>0.08785</td>\n",
       "      <td>0.05794</td>\n",
       "      <td>0.02360</td>\n",
       "      <td>0.02402</td>\n",
       "      <td>...</td>\n",
       "      <td>16.92</td>\n",
       "      <td>88.12</td>\n",
       "      <td>566.9</td>\n",
       "      <td>0.1314</td>\n",
       "      <td>0.1607</td>\n",
       "      <td>0.09385</td>\n",
       "      <td>0.08224</td>\n",
       "      <td>0.2775</td>\n",
       "      <td>0.09464</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>861648</td>\n",
       "      <td>B</td>\n",
       "      <td>14.620</td>\n",
       "      <td>24.02</td>\n",
       "      <td>94.57</td>\n",
       "      <td>662.7</td>\n",
       "      <td>0.08974</td>\n",
       "      <td>0.08606</td>\n",
       "      <td>0.03102</td>\n",
       "      <td>0.02957</td>\n",
       "      <td>...</td>\n",
       "      <td>29.11</td>\n",
       "      <td>102.90</td>\n",
       "      <td>803.7</td>\n",
       "      <td>0.1115</td>\n",
       "      <td>0.1766</td>\n",
       "      <td>0.09189</td>\n",
       "      <td>0.06946</td>\n",
       "      <td>0.2522</td>\n",
       "      <td>0.07246</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>914062</td>\n",
       "      <td>M</td>\n",
       "      <td>18.010</td>\n",
       "      <td>20.56</td>\n",
       "      <td>118.40</td>\n",
       "      <td>1007.0</td>\n",
       "      <td>0.10010</td>\n",
       "      <td>0.12890</td>\n",
       "      <td>0.11700</td>\n",
       "      <td>0.07762</td>\n",
       "      <td>...</td>\n",
       "      <td>26.06</td>\n",
       "      <td>143.40</td>\n",
       "      <td>1426.0</td>\n",
       "      <td>0.1309</td>\n",
       "      <td>0.2327</td>\n",
       "      <td>0.25440</td>\n",
       "      <td>0.14890</td>\n",
       "      <td>0.3251</td>\n",
       "      <td>0.07625</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>8712729</td>\n",
       "      <td>M</td>\n",
       "      <td>16.780</td>\n",
       "      <td>18.80</td>\n",
       "      <td>109.30</td>\n",
       "      <td>886.3</td>\n",
       "      <td>0.08865</td>\n",
       "      <td>0.09182</td>\n",
       "      <td>0.08422</td>\n",
       "      <td>0.06576</td>\n",
       "      <td>...</td>\n",
       "      <td>26.30</td>\n",
       "      <td>130.70</td>\n",
       "      <td>1260.0</td>\n",
       "      <td>0.1168</td>\n",
       "      <td>0.2119</td>\n",
       "      <td>0.23180</td>\n",
       "      <td>0.14740</td>\n",
       "      <td>0.2810</td>\n",
       "      <td>0.07228</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>865468</td>\n",
       "      <td>B</td>\n",
       "      <td>13.370</td>\n",
       "      <td>16.39</td>\n",
       "      <td>86.10</td>\n",
       "      <td>553.5</td>\n",
       "      <td>0.07115</td>\n",
       "      <td>0.07325</td>\n",
       "      <td>0.08092</td>\n",
       "      <td>0.02800</td>\n",
       "      <td>...</td>\n",
       "      <td>22.75</td>\n",
       "      <td>91.99</td>\n",
       "      <td>632.1</td>\n",
       "      <td>0.1025</td>\n",
       "      <td>0.2531</td>\n",
       "      <td>0.33080</td>\n",
       "      <td>0.08978</td>\n",
       "      <td>0.2048</td>\n",
       "      <td>0.07628</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>891703</td>\n",
       "      <td>B</td>\n",
       "      <td>11.850</td>\n",
       "      <td>17.46</td>\n",
       "      <td>75.54</td>\n",
       "      <td>432.7</td>\n",
       "      <td>0.08372</td>\n",
       "      <td>0.05642</td>\n",
       "      <td>0.02688</td>\n",
       "      <td>0.02280</td>\n",
       "      <td>...</td>\n",
       "      <td>25.75</td>\n",
       "      <td>84.35</td>\n",
       "      <td>517.8</td>\n",
       "      <td>0.1369</td>\n",
       "      <td>0.1758</td>\n",
       "      <td>0.13160</td>\n",
       "      <td>0.09140</td>\n",
       "      <td>0.3101</td>\n",
       "      <td>0.07007</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "527  91813702         B       12.340         12.27           78.94      468.5   \n",
       "524    917897         B        9.847         15.68           63.00      293.2   \n",
       "423    906878         B       13.660         19.13           89.46      575.3   \n",
       "6      844359         M       18.250         19.98          119.60     1040.0   \n",
       "418    906024         B       12.700         12.17           80.88      495.0   \n",
       "90     861648         B       14.620         24.02           94.57      662.7   \n",
       "492    914062         M       18.010         20.56          118.40     1007.0   \n",
       "167   8712729         M       16.780         18.80          109.30      886.3   \n",
       "124    865468         B       13.370         16.39           86.10      553.5   \n",
       "293    891703         B       11.850         17.46           75.54      432.7   \n",
       "\n",
       "     smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "527          0.09003           0.06307         0.02958              0.02647   \n",
       "524          0.09492           0.08419         0.02330              0.02416   \n",
       "423          0.09057           0.11470         0.09657              0.04812   \n",
       "6            0.09463           0.10900         0.11270              0.07400   \n",
       "418          0.08785           0.05794         0.02360              0.02402   \n",
       "90           0.08974           0.08606         0.03102              0.02957   \n",
       "492          0.10010           0.12890         0.11700              0.07762   \n",
       "167          0.08865           0.09182         0.08422              0.06576   \n",
       "124          0.07115           0.07325         0.08092              0.02800   \n",
       "293          0.08372           0.05642         0.02688              0.02280   \n",
       "\n",
       "        ...       texture_worst  perimeter_worst  area_worst  \\\n",
       "527     ...               19.27            87.22       564.9   \n",
       "524     ...               22.99            74.32       376.5   \n",
       "423     ...               25.50           101.40       708.8   \n",
       "6       ...               27.66           153.20      1606.0   \n",
       "418     ...               16.92            88.12       566.9   \n",
       "90      ...               29.11           102.90       803.7   \n",
       "492     ...               26.06           143.40      1426.0   \n",
       "167     ...               26.30           130.70      1260.0   \n",
       "124     ...               22.75            91.99       632.1   \n",
       "293     ...               25.75            84.35       517.8   \n",
       "\n",
       "     smoothness_worst  compactness_worst  concavity_worst  \\\n",
       "527            0.1292             0.2074          0.17910   \n",
       "524            0.1419             0.2243          0.08434   \n",
       "423            0.1147             0.3167          0.36600   \n",
       "6              0.1442             0.2576          0.37840   \n",
       "418            0.1314             0.1607          0.09385   \n",
       "90             0.1115             0.1766          0.09189   \n",
       "492            0.1309             0.2327          0.25440   \n",
       "167            0.1168             0.2119          0.23180   \n",
       "124            0.1025             0.2531          0.33080   \n",
       "293            0.1369             0.1758          0.13160   \n",
       "\n",
       "     concave points_worst  symmetry_worst  fractal_dimension_worst  \\\n",
       "527               0.10700          0.3110                  0.07592   \n",
       "524               0.06528          0.2502                  0.09209   \n",
       "423               0.14070          0.2744                  0.08839   \n",
       "6                 0.19320          0.3063                  0.08368   \n",
       "418               0.08224          0.2775                  0.09464   \n",
       "90                0.06946          0.2522                  0.07246   \n",
       "492               0.14890          0.3251                  0.07625   \n",
       "167               0.14740          0.2810                  0.07228   \n",
       "124               0.08978          0.2048                  0.07628   \n",
       "293               0.09140          0.3101                  0.07007   \n",
       "\n",
       "     Unnamed: 32  \n",
       "527          NaN  \n",
       "524          NaN  \n",
       "423          NaN  \n",
       "6            NaN  \n",
       "418          NaN  \n",
       "90           NaN  \n",
       "492          NaN  \n",
       "167          NaN  \n",
       "124          NaN  \n",
       "293          NaN  \n",
       "\n",
       "[10 rows x 33 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's load the cancer dataset, shuffle it and  speratre into train and test set\n",
    "dataset = pd.read_csv('./datasets/cancer_dataset.csv')\n",
    "# print the number of rows in the data set\n",
    "number_of_rows = len(dataset)\n",
    "print(\"total samples: {}\".format(number_of_rows))\n",
    "total_positive_samples = np.sum(dataset['diagnosis'].values == 'M')\n",
    "print(\"total positive sampels (M): {}, total negative samples (B): {}\".format(total_positive_samples, number_of_rows - total_positive_samples))\n",
    "num_train = int(0.8 * number_of_rows)\n",
    "# reminder, the data looks like this\n",
    "# dataset.head(10) # the dataset is ordered by the diagnosis\n",
    "dataset.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total training samples: 455, total test samples: 114\n"
     ]
    }
   ],
   "source": [
    "# prepare the dataset\n",
    "# we will take the first 2 features as our data (X) and the diagnosis as labels (y)\n",
    "x = dataset[['radius_mean', 'texture_mean', 'concavity_mean']].values\n",
    "y = dataset['diagnosis'].values == 'M'  # 1 for Malignat, 0 for Benign\n",
    "# shuffle\n",
    "rand_gen = np.random.RandomState(0)\n",
    "shuffled_indices = rand_gen.permutation(np.arange(len(x)))\n",
    "\n",
    "x_train = x[shuffled_indices[:num_train]]\n",
    "y_train = y[shuffled_indices[:num_train]]\n",
    "x_test = x[shuffled_indices[num_train:]]\n",
    "y_test = y[shuffled_indices[num_train:]]\n",
    "\n",
    "# pre-process - standartization\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train)\n",
    "x_train = scaler.transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n",
    "\n",
    "print(\"total training samples: {}, total test samples: {}\".format(num_train, number_of_rows - num_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.9385964912280702\n",
      "RandomForestClassifier 0.9473684210526315\n",
      "SVC 0.9473684210526315\n",
      "VotingClassifier 0.9473684210526315\n"
     ]
    }
   ],
   "source": [
    "# hard voting\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "random_state = 38\n",
    "\n",
    "log_clf = LogisticRegression(random_state=random_state)\n",
    "rnd_clf = RandomForestClassifier(random_state=random_state)\n",
    "svm_clf = SVC(random_state=random_state)\n",
    "\n",
    "voting_clf = VotingClassifier(estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)], voting='hard')\n",
    "# voting_clf.fit(x_train, y_train)\n",
    "\n",
    "# let's look at each classifier's accuracy on the test set\n",
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.9385964912280702\n",
      "RandomForestClassifier 0.9473684210526315\n",
      "SVC 0.9473684210526315\n",
      "VotingClassifier 0.9473684210526315\n"
     ]
    }
   ],
   "source": [
    "# soft voting\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "random_state = 38\n",
    "\n",
    "log_clf = LogisticRegression(random_state=random_state)\n",
    "rnd_clf = RandomForestClassifier(random_state=random_state)\n",
    "svm_clf = SVC(probability=True, random_state=random_state)\n",
    "\n",
    "voting_clf = VotingClassifier(estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)], voting='soft')\n",
    "# voting_clf.fit(x_train, y_train)\n",
    "\n",
    "# let's look at each classifier's accuracy on the test set\n",
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <img src=\"https://img.icons8.com/plasticine/100/000000/briefcase.png\" style=\"height:50px;display:inline\"> Bagging (& Pasting)\n",
    "* Another approach to get a diverse set of classifiers is to use the **same training algorithm** for every predictor, but to train them on **different random subsets of the training set**.\n",
    "* When sampling is performed **with replacement** this method is called **bagging** (which is a short for *bootstrap aggregating*).\n",
    "    * In sampling **with replacement**, each sample unit of the population can occur one or more times in the sample.\n",
    "    * In statistics, resampling with replcement is called *boostrapping*.\n",
    "* When sampling is performed **without replacement** this method is called **pasting**.\n",
    "* Thus, both bagging and pasting allow training instances to be sampled several times across multiple predictors, but only bagging allows training instances to be sampled several times for the same predictor.\n",
    "* Illustartion: <img src=\"./assets/tut_11_bagging_pasting.png\" style=\"height:200px\">\n",
    "    * Image from <a href=\"https://github.com/SoojungHong/MachineLearning/wiki/Random-Forest\">ML-Random Forest by SoojungHong</a>\n",
    "* Once all predictors are trained, the ensemble can make a prediction for a new instance by collecting all the predictions of all the predictors. It usually decicided by *hard voting* or average for regression.\n",
    "* Each individual predictor has a higher bias than if it were trained on the original training set, but the aggregation **reduces both bias and variance**.\n",
    "    * It is common to see that the ensemble has a similar bias but a lower variance than a single predictor trained on the original training set.\n",
    "\n",
    "* **Bootstrap Algorithm**:\n",
    "    * Denote the original sample: $ L_N = (x_1, x_2, ..., x_N) $\n",
    "    * Repeat $M$ times:\n",
    "        * Generate a sample $L_k$ of size $k$ from $L_N$ by sampling *with replacement*.\n",
    "        * Compute $h$ from $L_k$ (that is, train a predictor $h$ using $L_k$).\n",
    "    * Denote the bootstrap values $H=(h^1, h^2, ..., h^M)$\n",
    "        * Use these values for calculating all the quantities of interest.\n",
    "* **Bagging**:\n",
    "    * Train each model with a random training set (bootsrap).\n",
    "    * Each model in the ensemble has an **equal weight** in the voting.\n",
    "    * Finally: $$ H(x) = sign(h^1(x) +h^2(x) +... +h^M(x)) $$\n",
    "        * One classifier can be wrong as long as the others are correct (*hard voting*) <img src=\"./assets/tut_11_bagging_1.jpg\" style=\"height:200px\">\n",
    "        * Since given equal weight, this may cause problems when there is overlap. <img src=\"./assets/tut_11_bagging_2.jpg\" style=\"height:200px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bagging accuracy: 0.921, pasting accuracy: 0.930\n"
     ]
    }
   ],
   "source": [
    "# bagging\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# note: BaggingClassifiers will automatically perform 'soft voting' instead of 'hard voting'\n",
    "# if the base classifier can estimate class probabilities (i.e. if it has a \"predict_proba()\" method).\n",
    "\n",
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(),\n",
    "    n_estimators=500,\n",
    "    max_samples=100,\n",
    "    bootstrap=True,\n",
    "    n_jobs=1)\n",
    "bag_clf.fit(x_train, y_train)\n",
    "y_pred = bag_clf.predict(x_test)\n",
    "bag_acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# pasting\n",
    "pas_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(),\n",
    "    n_estimators=500,\n",
    "    max_samples=100,\n",
    "    bootstrap=False,\n",
    "    n_jobs=1)\n",
    "pas_clf.fit(x_train, y_train)\n",
    "y_pred = pas_clf.predict(x_test)\n",
    "pas_acc = accuracy_score(y_test, y_pred)\n",
    "print(\"bagging accuracy: {:.3f}, pasting accuracy: {:.3f}\".format(bag_acc, pas_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <img src=\"https://img.icons8.com/bubbles/100/000000/rocket.png\" style=\"height:50px;display:inline\"> Boosting\n",
    "* **Boosting** (also *hypothesis boosting*) - any Ensemble method that can combine several weak learners into a a strong learner. In boosting methods, predictors are trained **sequentially**, each trying to correct its predecessor.\n",
    "    * Weak Learner - as before, the error rate is slighty better then flipping a coin\n",
    "    * We also define:\n",
    "        * $h$ is binary classifier such that $h \\in \\{-1, 1\\}$\n",
    "        * Error rate $Err \\in [0,1]$\n",
    "* The principal difference between boosting and the committe methods is that in boosting, the base classifiers are **trained in sequence**.\n",
    "* Each base classifier is trained using a **weighted form of the dataset**, in which the weight coefficient associated with each data point depends on the performance of the previous classifiers.\n",
    "    * In particular, points that are misclassified by one of the base classifiers are given greater weight when used to train the next classifier in the sequence.\n",
    "* Once all the classifiers have been trained, their predictions are then combined through a **weighted majority voting** scheme.\n",
    "* Visually: <img src=\"./assets/tut_11_boosting_1.jpg\" style=\"height:300px\"><br><img src=\"./assets/tut_11_boosting_2.jpg\" style=\"height:400px\">\n",
    "* There are many boosting methods, but we will examine one of the most popular one called *AdaBoost*.\n",
    "\n",
    "### <img src=\"https://img.icons8.com/dusk/64/000000/wired-network.png\" style=\"height:50px;display:inline\"> AdaBoost\n",
    "* The idea of AdaBoost is to give more attention to training instances that the predecessor underfitted. This leads to a predictor that focuses more and more on the hard cases.\n",
    "* The sequential learning in Boosting seems similar to Gradient Descent, only in AdaBoost predictors are added to the ensemble in order to make it better where in GD, a single predictor's paramerters are optimized to minimize an objective function.\n",
    "* Once all predictors are trained, the ensemble makes predictions by assigning different weights to each predictor, depending on their **overall accuracy on the weighted training set**.\n",
    "* Definitions:\n",
    "    * Class labels are $\\{-1, 1\\}$\n",
    "    * $m$ - number of samples in the training dataset\n",
    "    * The weighted error rate of the $t^{th}$ predictor: $$ \\epsilon_t =\\sum _{i=1}^m w^{(i)} \\cdot \\mathbb{1}(\\hat{y}_t^{(i)} \\neq y^{(i)})$$ In the more general case where the weights are not normalized to 1: $$ \\epsilon_t =\\frac{\\sum _{i=1}^m w^{(i)} \\cdot \\mathbb{1}(\\hat{y}_t^{(i)} \\neq y^{(i)})}{\\sum _{i=1}^m w^{(i)}} $$\n",
    "        * $\\hat{y}_t^{(i)}$ is the $t^{th}$ predictor's prediction for the $i^{th}$ instance.\n",
    "    * The predictors weight of the $t^{th}$ predictor: $$ \\alpha_t = \\eta \\ln \\frac{1 - \\epsilon_t}{\\epsilon_t} $$\n",
    "        * $\\eta$ it the learning rate hyperparameter, e.g. $\\frac{1}{2}$ or 1.\n",
    "        * The more accurate the predictor is, the more weight the predictor will be given.\n",
    "    * The update rule: for $i = 1,2, ..., m $ $$ w^{(i)} \\leftarrow \\begin{cases} w^{(i)}e^{-\\alpha_t} & \\quad \\text{if } \\hat{y}_t^{(i)} = y^{(i)}  \\\\ w^{(i)}e^{\\alpha_t} & \\quad \\text{if } \\hat{y}_t^{(i)} \\neq y^{(i)} \\end{cases} = w^{(i)}e^{-\\alpha_t\\cdot y^{(i)} \\cdot \\hat{y}_t^{(i)}}$$\n",
    "        * Once all the weights were calculated, they are summed. The sum is denoted $Z_t$. Then, all the weights are normalized by dividing each weight by $Z_t$.\n",
    "* Stopping criteria:\n",
    "    * The desiered number of predictors is reached.\n",
    "    * A perfert predictor is found.\n",
    "* **The AdaBoost Algorithm**:\n",
    "    * Initialize the data weights coefficients $\\{w^{(i)}\\}_{i=1}^m$: $$ w^{(i)} = \\frac{1}{m}, \\forall i= 1,2,...,m $$\n",
    "    * For $t = 1,...,T$:\n",
    "        * Fit a weak classifier $h_t(x)$ (which makes predictions $\\hat{y}_t$) to the weighted training data and calculate the weighted error rate: $$ \\epsilon_t =\\frac{\\sum _{i=1}^m w^{(i)} \\cdot \\mathbb{1}(\\hat{y}_t^{(i)} \\neq y^{(i)})}{\\sum _{i=1}^m w^{(i)}} $$\n",
    "        * Choose $\\alpha_t$ (default $\\eta=\\frac{1}{2}$): $$ \\alpha_t = \\frac{1}{2} \\ln \\frac{1 - \\epsilon_t}{\\epsilon_t} $$\n",
    "        * Update the weights: for $i = 1,2, ..., m $ $$ w^{(i)} \\leftarrow \\begin{cases} w^{(i)}e^{-\\alpha_t} & \\quad \\text{if } \\hat{y}_t^{(i)} = y^{(i)}  \\\\ w^{(i)}e^{\\alpha_t} & \\quad \\text{if } \\hat{y}_t^{(i)} \\neq y^{(i)} \\end{cases} = w^{(i)}e^{-\\alpha_t\\cdot y^{(i)} \\cdot \\hat{y}_t^{(i)}}$$\n",
    "        * Normalize the weights: for $i = 1,2, ..., m $ $$ w^{(i)} \\leftarrow \\frac{w^{(i)}}{Z_t} $$\n",
    "            * $Z_t = \\sum_{i=1}^m w^{(i)}$\n",
    "    * Use predictions using the final model, which is given by: $$ H(x) = sign(\\sum_{i=1}^T \\alpha_th_t(x)) $$\n",
    "        \n",
    "#### <img src=\"https://img.icons8.com/nolan/64/000000/down2.png\" style=\"height:30px;display:inline\"> Exponential Loss\n",
    "* So far, the loss functions we have seen:\n",
    "    * 0-1 loss\n",
    "    * Hinge loss\n",
    "    * Log loss\n",
    "* Unlike previously learnt classifiers, AdaBoost minimzes the exponential loss.\n",
    "* All lossess upper bound the 0-1 loss and act as differentiable surrogate loss functions.\n",
    "* <img src=\"./assets/tut_11_exp_loss.jpg\" style=\"height:200px\">\n",
    "* Optimizing the exponential loss:\n",
    "    * As shown in class, the training error is upper bounded by $H$: $$ \\frac{1}{m} \\sum_i^m \\mathbb{1}(H(x_i) \\neq y_i) \\leq \\prod_{t=1}^T Z_t  $$\n",
    "        * $Z_t = \\sum_i w_t^{(i)} e^{-\\alpha_t y_i h_t(x_i)} $\n",
    "    * At each round we minimize $Z_t$ by:\n",
    "        * Choosing the optimal $h_t$\n",
    "        * Finding the optimal $\\alpha_t$\n",
    "        * $$ \\frac{dZ}{d\\alpha} = -\\sum_{i=1}^m w^{(i)} y_ih(x_i) e^{-\\alpha y_ih(x_i)} = 0 $$ $$ -\\sum_{i:y_i=h(x_i)}w^{(i)} e^{-\\alpha} + \\sum_{i: y_i \\neq h(x_i)} w^{(i)} e^{\\alpha} = 0 $$ $$ -e^{-\\alpha} (1-\\epsilon) +e^{\\alpha} \\epsilon = 0 $$ $$ \\rightarrow \\alpha_t = \\frac{1}{2} \\ln \\frac{1 - \\epsilon_t}{\\epsilon_t} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/dusk/64/000000/classroom.png\" style=\"height:50px;display:inline\"> Boosting Example By Hand\n",
    "\n",
    "Moses is a student who wants to avoid hard courses. \n",
    "\n",
    "In order to achieve this he wants to build a classifier that classifies courses as \"easy\" or \"hard\".\n",
    "\n",
    "He decides to classify courses' hardness by using AdaBoost with decision trees stumps (decision trees with max depth of 1) on the following data:\n",
    "\n",
    "| <center> Course ID</center>| <center> Hard </center> | <center> Final Exam </center> | <center>Theoretical </center>  | <center> Midterm </center>| <center> 236* </center> | <center> Number of HW </center>  \n",
    "| --- | --- | --- | --- | --- | --- | --- |\n",
    "|<center> 1</center> | <center> Y </center> | <center>Y </center>|<center> N </center> | <center> Y </center>| <center>N </center>| <center> 5</center>|\n",
    "|<center> 2</center> | <center> Y </center> | <center>N </center>|<center> Y </center> | <center> Y </center>| <center>N </center>| <center> 5</center>|\n",
    "|<center> 3</center> | <center> Y </center> | <center>N </center>|<center> Y </center> | <center> N </center>| <center>Y </center>| <center> 1</center>|\n",
    "|<center> 4</center> | <center> Y </center> | <center>N </center>|<center> Y </center> | <center> N </center>| <center>N </center>| <center> 3</center>|\n",
    "|<center> 5</center> | <center> Y </center> | <center>N </center>|<center> Y </center> | <center> N </center>| <center>N </center>| <center> 5</center>|\n",
    "|<center> 6</center> | <center> Y </center> | <center>Y </center>|<center> N </center> | <center> Y </center>| <center>N </center>| <center> 5</center>|\n",
    "|<center> 7</center> | <center> Y </center> | <center>Y </center>|<center> N </center> | <center> Y </center>| <center>N </center>| <center> 5</center>|\n",
    "|<center> 8</center> | <center> N </center> | <center>N </center>|<center> N </center> | <center> Y </center>| <center>Y </center>| <center> 1</center>|\n",
    "|<center> 9</center> | <center> N </center> | <center>N </center>|<center> Y </center> | <center> N </center>| <center>N </center>| <center> 1</center>|\n",
    "|<center> 10</center> | <center> Y </center> | <center>N </center>|<center> N </center> | <center> N </center>| <center>N </center>| <center> 5</center>|\n",
    "\n",
    "As a first step, he first determined for each possible classifier (including the trivial constant classifier), which of the data points were misclassfied.\n",
    "\n",
    "For example, for the first classifier which classfies courses as hard if they have a final exam, the classifier is wrong on samples 2,3,4 and 5.\n",
    "\n",
    "| <center> Classifier</center>| <center> Test </center> | <center> Value </center> | <center>Misclassified </center>  |\n",
    "| --- | --- | --- | --- |\n",
    "|<center> A</center> | <center> Final Exam </center> | <center>Y </center>|<center> 2,3,4,5 </center> |\n",
    "|<center> B</center> | <center> Theoretical </center> | <center>Y </center>|<center> 1,6,7,9 </center> |\n",
    "|<center> C</center> | <center> Midterm</center> | <center>Y </center>|<center> 3,4,5,8 </center> |\n",
    "|<center> D</center> | <center> Undergrduate </center> | <center>Y </center>|<center> 1,2,4,5,6,7,8 </center> |\n",
    "|<center> E</center> | <center> # HW > 2 </center> | <center>Y </center>|<center> 3,10 </center> |\n",
    "|<center> F</center> | <center> # HW > 4 </center> | <center>Y </center>|<center> 3,4,10 </center> |\n",
    "|<center> G</center> | <center> True (const) </center> | <center> </center>|<center> 8,9,10 </center> |\n",
    "|<center> H</center> | <center> Final Exam </center> | <center>N </center>|<center> 1,6,7,8,9,10 </center> |\n",
    "|<center> I</center> | <center> Theoretical </center> | <center>N </center>|<center> 2,3,4,5,8,10 </center> |\n",
    "|<center> J</center> | <center> Midterm</center> | <center>N </center>|<center> 1,2,6,7,9,10 </center> |\n",
    "|<center> K</center> | <center> Undergraduate </center> | <center>N </center>|<center> 3,9,10 </center> |\n",
    "|<center> L</center> | <center> # HW < 2 </center> | <center>Y </center>|<center> 1,2,4,5,6,7,8,9 </center> |\n",
    "|<center> M</center> | <center> # HW < 4 </center> | <center>Y </center>|<center> 1,2,5,6,7,8,9 </center> |\n",
    "|<center> N</center> | <center> False (const) </center> | <center> </center>|<center> 1,2,3,4,5,6,7 </center> |\n",
    "\n",
    "#### Consider only useful classifiers\n",
    "Only 6 classifeirs from the table above would ever be used because the other 8 make all the same error as one of the other classifiers and then make additional erros. For example, classifiers I and N do the same mistakes as A and add to that. The 6 useful classifiers are:\n",
    "\n",
    "| <center> Classifier</center>| <center> Test </center> | <center> Value </center> | <center>Misclassified </center>  |\n",
    "| --- | --- | --- | --- |\n",
    "|<center> A</center> | <center> Final Exam </center> | <center>Y </center>|<center> 2,3,4,5 </center> |\n",
    "|<center> B</center> | <center> Theoretical </center> | <center>Y </center>|<center> 1,6,7,9 </center> |\n",
    "|<center> C</center> | <center> Midterm</center> | <center>Y </center>|<center> 3,4,5,8 </center> |\n",
    "|<center> D</center> | <center> Undergrduate </center> | <center>Y </center>|<center> 1,2,4,5,6,7,8 </center> |\n",
    "|<center> E</center> | <center> # HW > 2 </center> | <center>Y </center>|<center> 3,10 </center> |\n",
    "|<center> G</center> | <center> True (const) </center> | <center> </center>|<center> 8,9,10 </center> |\n",
    "\n",
    "\n",
    "#### AdaBoost\n",
    "* We will now perform AdaBoost by calculating the weights at each iteration.\n",
    "* We will calculate the 10 weights, the classification $h$, the error and $\\alpha$.\n",
    "* If there is a tie, we break it by choosing the classifier that is higher on the list (lexicographical order)\n",
    "* Note: in this example we assume that the weights of the data points do not affect the clasification and are just meant to calculate the final weight of each classifier.\n",
    "\n",
    "#### Round 1\n",
    "* Each weight is given the same value: $\\frac{1}{m} = \\frac{1}{10}$\n",
    "* Since classifier $E$ is the most accurate, it will serve as the classifier.\n",
    "* The weight error rate of classifier $E$ is $\\epsilon_E = \\frac{2}{10}$\n",
    "* Thus: $\\alpha_E = \\frac{1}{2}\\ln \\frac{1 - \\epsilon_E}{\\epsilon_E} = \\frac{1}{2} \\ln (4)$\n",
    "\n",
    "| <center> Parameters &nbsp; &nbsp; &nbsp; &nbsp;</center>| <center> Round 1 </center> | <center> Round 2 </center> | <center> Round 3 </center>  |\n",
    "| ----- | --- | --- | --- |\n",
    "|<center> w1</center> | <center> $\\frac{1}{10}$ </center> | <center> </center>|<center> </center> |\n",
    "|<center> w2</center> | <center> $\\frac{1}{10}$ </center> | <center> </center>|<center> </center> |\n",
    "|<center> w3</center> | <center> $\\frac{1}{10}$ </center> | <center> </center>|<center> </center> |\n",
    "|<center> w4</center> | <center> $\\frac{1}{10}$ </center> | <center> </center>|<center> </center> |\n",
    "|<center> w5</center> | <center> $\\frac{1}{10}$ </center> | <center> </center>|<center> </center> |\n",
    "|<center> w6</center> | <center> $\\frac{1}{10}$ </center> | <center> </center>|<center> </center> |\n",
    "|<center> w7</center> | <center> $\\frac{1}{10}$ </center> | <center> </center>|<center> </center> |\n",
    "|<center> w8</center> | <center> $\\frac{1}{10}$ </center> | <center> </center>|<center> </center> |\n",
    "|<center> w9</center> | <center> $\\frac{1}{10}$ </center> | <center> </center>|<center> </center> |\n",
    "|<center> w10</center> | <center> $\\frac{1}{10}$ </center> | <center> </center>|<center> </center> |\n",
    "|<center> $h$</center> | <center> $E$ </center> | <center> </center>|<center> </center> |\n",
    "|<center> Err - $\\epsilon$</center> | <center> $\\frac{2}{10}$ </center> | <center> </center>|<center> </center> |\n",
    "|<center> $$\\alpha = \\frac{1}{2}\\ln \\frac{1 - \\epsilon}{\\epsilon} $$</center> | <center> $\\frac{1}{2} \\ln (4)$ </center> | <center> </center>|<center> </center> |\n",
    "\n",
    "#### AdaBoost - calculating the new weights\n",
    "* Recall that the un-normalized weights update: $$ \\tilde{w}_{t+1}^{(i)} = w_t^{(i)} e^{-\\alpha_ty_ih_t(x_i)} $$\n",
    "* For the correctly classified data points (8 points): $$ \\tilde{w}_{t+1}^{(i)} = \\frac{1}{10}e^{-\\frac{1}{2}\\ln (4)} = \\frac{1}{10} \\cdot \\frac{1}{2} = \\frac{1}{20} $$\n",
    "* For the incorrectly classified data points (2 points): $$ \\tilde{w}_{t+1}^{(i)} = \\frac{1}{10}e^{\\frac{1}{2}\\ln (4)} = \\frac{1}{10} \\cdot 2 = \\frac{1}{5} $$\n",
    "* Calculate the normalization factor: $$ Z_t = 8 \\cdot \\frac{1}{20} + 2 \\cdot \\frac{1}{5} = \\frac{4}{5}  $$\n",
    "* The final weights after normalization:\n",
    "    * Correct: $w_{t+1}^{(i)} = \\frac{1}{20} \\cdot \\frac{5}{4} = \\frac{1}{16}$\n",
    "    * Incorrect: $w_{t+1}^{(i)} = \\frac{1}{5} \\cdot \\frac{5}{4} = \\frac{1}{4}$\n",
    "\n",
    "Similarly, we fill in the rest of the table:\n",
    "\n",
    "\n",
    "| <center> Parameters &nbsp; &nbsp; &nbsp; &nbsp;</center>| <center> Round 1 </center> | <center> Round 2 </center> | <center> Round 3 </center>  |\n",
    "| ----- | --- | --- | --- |\n",
    "|<center> w1</center> | <center> $\\frac{1}{10}$ </center> | <center> $\\frac{1}{16}$</center>|<center>$\\frac{3}{24}$ </center> |\n",
    "|<center> w2</center> | <center> $\\frac{1}{10}$ </center> | <center>$\\frac{1}{16}$ </center>|<center> $\\frac{1}{24}$</center> |\n",
    "|<center> w3</center> | <center> $\\frac{1}{10}$ </center> | <center>$\\frac{4}{16}$ </center>|<center>$\\frac{4}{24}$ </center> |\n",
    "|<center> w4</center> | <center> $\\frac{1}{10}$ </center> | <center>$\\frac{1}{16}$ </center>|<center> $\\frac{1}{24}$</center> |\n",
    "|<center> w5</center> | <center> $\\frac{1}{10}$ </center> | <center>$\\frac{1}{16}$ </center>|<center>$\\frac{1}{24}$ </center> |\n",
    "|<center> w6</center> | <center> $\\frac{1}{10}$ </center> | <center>$\\frac{1}{16}$ </center>|<center>$\\frac{3}{24}$ </center> |\n",
    "|<center> w7</center> | <center> $\\frac{1}{10}$ </center> | <center>$\\frac{1}{16}$ </center>|<center>$\\frac{3}{24}$ </center> |\n",
    "|<center> w8</center> | <center> $\\frac{1}{10}$ </center> | <center>$\\frac{1}{16}$ </center>|<center>$\\frac{1}{24}$ </center> |\n",
    "|<center> w9</center> | <center> $\\frac{1}{10}$ </center> | <center> $\\frac{1}{16}$</center>|<center>$\\frac{3}{24}$ </center> |\n",
    "|<center> w10</center> | <center> $\\frac{1}{10}$ </center> | <center>$\\frac{4}{16}$ </center>|<center>$\\frac{4}{24}$ </center> |\n",
    "|<center> $h$</center> | <center> $E$ </center> | <center> $B$ </center>|<center> $A$ </center> |\n",
    "|<center> Err - $\\epsilon$</center> | <center> $\\frac{2}{10}$ </center> | <center> $\\frac{1}{4}$ </center>|<center> $\\frac{7}{24}$ </center> |\n",
    "|<center> $$\\alpha = \\frac{1}{2}\\ln \\frac{1 - \\epsilon}{\\epsilon} $$</center> | <center> $\\frac{1}{2} \\ln (4)$ </center> | <center>  $\\frac{1}{2} \\ln (3)$ </center>|<center> $\\frac{1}{2} \\ln \\frac{17}{7}$ </center> |\n",
    "\n",
    "#### AdaBoost - Putting the classifiers together\n",
    "* The final classifier for 3 rounds of Boosting: $$ H(x) = sign(\\frac{1}{2} \\ln (4) \\cdot h_E(x) + \\frac{1}{2} \\ln (3) \\cdot h_B(x) + \\frac{1}{2} \\ln \\frac{17}{7} \\cdot h_A(x)) $$\n",
    "    * $h_c(x)$ returns +1 or -1 for $c=E,B,A$\n",
    "* The data points that the final classifier is correct about them:\n",
    "    * Since $\\alpha_E, \\alpha_B > \\alpha_A$ - it is just a *majority vote*\n",
    "    * Only one example (3) is misclassified\n",
    "    \n",
    "    \n",
    "### AdaBoost in Scikit-Learn\n",
    "* Scikit-Learn uses a multiclass version of AdaBoost called *SAMME* (Stagewise Additive Modeling using a Multiclass Exponential loss function).\n",
    "    * When there are just 2 classes, SAMME is equivalent to AdaBoost.\n",
    "    * If the predictors can estimate class probabilities (i.e. they have a `predict_proba()` method), Scikit-Learn can use a variant of SAMME called *SAMMER* (R for \"Real\"), which relies on class probabilities rather than predictions and generally performs better.\n",
    "    \n",
    "* The following code trains an AdaBoost classifier on 600 Decision Stumps.\n",
    "* Note: if the AdaBoost classifier is **overfitting** the training set, a good regularization may be reducing the number of estimators or more strongly regularize the base classifier.\n",
    "* An important drawback to sequential learning is that it cannot be parallelized, since each predictor can only be trained after the previous predictor has been trained and evaluated. Thus, it does not scale as well as bagging or pasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adaboost accuracy: 0.930\n"
     ]
    }
   ],
   "source": [
    "# AdaBoost\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "ada_clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1), n_estimators=600, algorithm=\"SAMME.R\", learning_rate=0.5)\n",
    "ada_clf.fit(x_train, y_train)\n",
    "y_pred = ada_clf.predict(x_test)\n",
    "ada_acc = accuracy_score(y_test, y_pred)\n",
    "print(\"adaboost accuracy: {:.3f}\".format(ada_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <img src=\"https://img.icons8.com/dusk/64/000000/prize.png\" style=\"height:50px;display:inline\"> Credits\n",
    "* Icons from <a href=\"https://icons8.com/\">Icon8.com</a> - https://icons8.com\n",
    "* Datasets from <a href=\"https://www.kaggle.com/\">Kaggle</a> - https://www.kaggle.com/\n",
    "* Examples and code snippets were taken from <a href=\"http://shop.oreilly.com/product/0636920052289.do\">\"Hands-On Machine Learning with Scikit-Learn and TensorFlow\"</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
