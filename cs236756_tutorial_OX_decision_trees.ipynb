{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img src=\"https://img.icons8.com/dusk/64/000000/mind-map.png\" style=\"height:50px;display:inline\"> CS 236756 - Technion - Intro to Machine Learning\n",
    "#### Tal Daniel\n",
    "## Tutorial 07 - Decision Trees\n",
    "\n",
    "* Based upon *Pattern Classification*, chapter 8\n",
    "\n",
    "### <img src=\"https://img.icons8.com/bubbles/50/000000/checklist.png\" style=\"height:50px;display:inline\"> Agenda\n",
    "\n",
    "* Motivation\n",
    "* How Decision Trees Work\n",
    "* Building a Decision Tree\n",
    "    * Single Value & Multi Value\n",
    "    * Impurity Metrics\n",
    "        * Entropy\n",
    "        * Gini\n",
    "    * Prunning\n",
    "* Example on the Titanic Dataset\n",
    "* Example on the Iris Dataset (Course Website)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for the tutorial\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from sklearn.datasets import load_iris\n",
    "import re  # regex\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <img src=\"https://img.icons8.com/dusk/64/000000/confetti.png\" style=\"height:50px;display:inline\"> Motivation\n",
    "* Decision Trees are ML algorithms that can perform **both** classification and regression tasks, and even multioutput tasks.\n",
    "* They are very powerful algorithms, capable of fitting complex datasets.\n",
    "* They are a fundamental components of **Random Forests**, which are amongsts the most powerful ML algorithms today. \n",
    "* Decision Trees require **very little data** preparation - they do not require **feature scaling or centering**.\n",
    "\n",
    "### <img src=\"https://img.icons8.com/dusk/64/000000/trial-version.png\" style=\"height:50px;display:inline\"> Examples\n",
    "1. <a href=\"http://en.akinator.com/personnages/jeu\">**Akinator**</a> - uses some implementation of *Decision Trees*. <img src=\"./assets/tut_07_akinator.jpg\" style=\"height:200px\">\n",
    "2. **20 Questions** - one player is chosen to be the answerer. That person chooses a subject (object) but does not reveal this to the others. All other players are questioners. They each take turns asking a question which can be answered with a simple \"Yes\" or \"No.\"\n",
    "    * *Features* - different properties of a character we ask about\n",
    "    * *Label* - the identity of the character\n",
    "    * *Training Set* - the characters you have encountered in your life\n",
    "    * *Test/Inference* - play \"20 Questions\"\n",
    "    * A good strategy:\n",
    "        * Eliminate as many options as possible\n",
    "        * Simple questions\n",
    "        * Consider more likely characters (Leibniz isn't who most people think of, but in the Technion, this might not be the case...)\n",
    "    * Note: this game is a special case of clssification where *each sample is its own class*\n",
    "    \n",
    "### <img src=\"https://img.icons8.com/dusk/64/000000/deciduous-tree.png\" style=\"height:50px;display:inline\"> Decision Tree Visualization\n",
    "<img src=\"./assets/tut_07_vis.jpg\" style=\"height:400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/dusk/64/000000/untested.png\" style=\"height:50px;display:inline\"> Considerations for Building a Tree\n",
    "1. **Binary** or **Multi-Valued**\n",
    "    * Each split can be into 2 nodes or more than 2 nodes. Binary trees are easier to work with.\n",
    "    * Every *Polythetic* tree can be represented by an eqivalent *Monothetic* tree.\n",
    "    * For example, the following tree can be transformed to the tree in the previous section: <img src=\"./assets/tut_07_multitree.jpg\" style=\"height:300px\"> <br>\n",
    "    * Most algorithms use *binary* trees, mainly, we will focus on the **CART** algorithm which is implemented in the Scikit-Learn library.\n",
    "        * The CART algorithm produces only binary trees: nonleaf nodes **always** have 2 children (i.e. questions only have yes/no answers).\n",
    "2. Choosing the **Property** (=Feature) to Test - for a given point of splitting, what is the next feature the descision should build upon?\n",
    "    * *Simple* questions: each branch should consider **few features** (1 if it is binary)\n",
    "    * The first features to be chosen should be the ones that *eliminate* the most options\n",
    "        * Which is to say, **increase the certainty of the decision**\n",
    "        * Or, lead us to **nodes that are homogenous (pure)**\n",
    "3. **Stopping Criteria** - when should a node be declared a *leaf*?\n",
    "4. **Prunning** - if the tree becomes too large (and thus, overfitting to the train dataset), how, and to what length, it can be made smaller and simpler?\n",
    "\n",
    "### <img src=\"https://img.icons8.com/cotton/64/000000/defensive-wood-wall.png\" style=\"height:50px;display:inline\"> Decision Boundries (Also for Regression Tasks)\n",
    "* In regression problems, the data is continuous, and boundries have to be set such that the input falls within a certain range.\n",
    "* These decision boundries can easily drawn on a graph as in the following example: <img src=\"./assets/tut_07_boundries.jpg\" style=\"height:300px\">\n",
    "\n",
    "#### <img src=\"https://img.icons8.com/dusk/64/000000/statistics.png\" style=\"height:30px;display:inline\"> Decision Trees for Regression Tasks\n",
    "In this tutorial we deal with classification tasks, where the input can be continuous features, but what if we want to complete a regression task, that is, output a value and not a class?\n",
    "\n",
    "Decision Trees are also capable of performing these tasks. We will not get into it, but the idea is the same. Instead of predicting a class, the tree predicts a value which is determined by the mean of the values in that branch. The CART algorithm is almost the same except for instead of minimizing the impurity, the MSE is minimized. You can read more about it on Scikit-Learn's documentation, under `DecisionTreeRegressor`.\n",
    "\n",
    "### <img src=\"https://img.icons8.com/dusk/64/000000/deviation.png\" style=\"height:50px;display:inline\"> Estimating Class Probabilities (Class Conditional Probabality)\n",
    "* A Decision Tree can also estimate the probability that an instance belongs to a particular class $k$\n",
    "    * First, it traverses the tree to find the leaf node for this instance\n",
    "    * Then, it returns the ratio of *training* instances of class $k$ in this node\n",
    "    * In Scikit-Learn (after training) - `tree_clf.predict_proba([list of samples])`\n",
    "* The class-conditional probabilities $P(c|D)$ are estimated by fitting a categorical MLE: $$ p_i = P(c|D) = \\frac{1}{|D|} \\sum_{i \\in D} \\mathbb{1}(y_i =c) $$\n",
    "    * $c$ - class\n",
    "    * $D$ - Data\n",
    "    * Reminder - **Categorical Distribution** is a generalization of Bernoulli to multiple classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <img src=\"https://img.icons8.com/dusk/64/000000/clean.png\" style=\"height:50px;display:inline\"> Impurity & Impurity Metrics\n",
    "* **Simplicity** is the leading principle in the tree building process\n",
    "* Therfore, we seek an attribute $A$ at each node $S$ that makes the immediate descendents as \"pure\" as possible.\n",
    "* At each node $S$, the vector $P=(p_1, p_2, ..., p_k)$, represents the probability (ratio) of samples belonging to class $i$\n",
    "    * **Impurity** of node $S$ - $\\phi(P) \\geq 0$\n",
    "    * $\\phi(P)=0$ if all samples have the same label\n",
    "        * That is, a node is \"pure\" if all training instance s it applies to belong to the same class\n",
    "    * $\\phi(P)$ is *maximized* for **uniform distribution**\n",
    "    * $\\phi(P)$ is **symetric** w.r.t. $P$\n",
    "    * $\\phi(P)$ is *smooth*, that is, differentiable everywhere\n",
    "    \n",
    "    \n",
    "### <img src=\"https://img.icons8.com/dusk/64/000000/bar-chart.png\" style=\"height:50px;display:inline\"> Impurity Metrics\n",
    "\n",
    "#### Entropy\n",
    "* The concept of entropy originates in thermodynamics as a measure of *molecular disorder*: entropy approaches 0 when molecules are still and well ordered.\n",
    "* Later, it was introduced in the *information theory* domain, where the average information content of a message is measured (a reduction in entropy is call **information gain**). When all messages are identical, the entropy is zero.\n",
    "* In Machine Learning, it is used as an impurity measure - a set's entropy is 0 when it contains instances of only one class.\n",
    "* Given a system whose exact description is *unknown*, its entropy is defined as **the amout of information needed to exactly specify the state of the system**\n",
    "    * If the system has $k$ states, we would need $\\log k$ bits to represent them\n",
    "* Denote $p_{i, k}$ as the ratio of class $k$ instances among the training instances in the $i^{th}$ node, the entropy is: $$ H_i = \\sum_{k=1}^n p_{i,k} \\log\\frac{1}{p_{i,k}} = - \\sum_{k=1}^n p_{i,k} \\log p_{i,k} $$\n",
    "* <a href=\"http://colah.github.io/posts/2015-09-Visual-Information/\">A friendly introduction to Information Theory</a>\n",
    "* <img src=\"./assets/tut_07_entropy.jpg\" style=\"height:150px\">\n",
    "\n",
    "#### Gini Index\n",
    "* Gini is another common measure for impurity which can be intepreted as a *variance impurity*\n",
    "* Denote $p_{i, k}$ as the ratio of class $k$ instances among the training instances in the $i^{th}$ node, the GiniIndex is: $$ G_i(P) = \\sum_{k=1}^n p_{i,k} (1-p_{i,k}) = 1 - \\sum_{k=1}^n p_{i,k}^2 $$\n",
    "    * Reminder: given $X \\sim Bern(p) \\rightarrow Var(X) = p \\cdot (1-p)$\n",
    "* This is the expected error rate at node $N$ if the category label is selected randomly from class distribution present at $N$\n",
    "* <img src='./assets/tut_07_gini.jpg' style=\"height:150px\">\n",
    "\n",
    "#### Gini vs. Entropy\n",
    "* Scikit-Learn's default is **Gini**, but by changing the call to `criterion=\"entropy\"`, the Entropy will be used to build the tree\n",
    "* Most of the time, **both** lead to similar trees.\n",
    "* **Gini** is (slightly) faster to compute\n",
    "* However, sometimes they are different as Gini tends to isloate the most frequent class in its own branch of the tree, while Entropy tends to produce slightly more *balanced* trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <img src=\"https://img.icons8.com/doodle/96/000000/large-tree.png\" style=\"height:50px;display:inline\"> Choosing the Features/Attributes for the Splits\n",
    "* Choose the one that most decreases impurity (and thus, increases purity)\n",
    "* Given node $S$ with probabilities $P$ and discrete attribute $A$, the **impurity drop** is defined as: $$ \\Delta \\phi(S,A) = \\phi(S) - \\sum_{v \\in \\textit{Values}(A)} p_v \\phi(S_v) $$\n",
    "    * For the **Binary** case: $$ \\Delta \\phi(S,A) = \\phi(S) - p_L \\phi(S_L) - (1 - p_L) \\phi(S_R) $$\n",
    "* For the *Entropy* measure, this measure is called **Information Gain (IG)** (or Mutual Information - MI): $$ IG(P) = H(P) - \\sum_{v \\in \\textit{Values}(A)} p_aH(p_a)$$\n",
    "* For *Gini*, the measure is called **Gini Gain**\n",
    "* We wish to **maximize** the information gain on the impurity drop, which is eqivalent to **minimizing** the right expression.\n",
    "* Denote a single feature as $k$ and a threshold value as $t_k$ (for example, height $\\leq 30 $ cm) ,**the CART algorithm cost function** (which we wish to minimize): $$ J(k, t_k) = \\frac{m_{left}}{m}G_{left} + \\frac{m_{right}}{m}G_{right} = p_LG_{left} + p_R G_{right} $$ where $G_{left/right}$ is the measure of impurity of the left/right subset and $m_{left/right}$ is the number of instances in the left/right subset.\n",
    "* The tree building algorithm (CART) is *greedy*, it greedily searches for an optimum split at the top level, then repeats the process at each level. It does not check whether or not the split will lead to the lowest possible impurity several levels down. A greedy algorithm usually produces reasonably good solutions, but they are not guaranteed to be optimal. \n",
    "\n",
    "#### <img src=\"https://img.icons8.com/dusk/64/000000/task.png\" style=\"height:30px;display:inline\"> Exercise 1 - Decsion Tree by Hand\n",
    "As of September 2012, 800 extrasolar planets have been identified in our galaxy. Supersecret surveying spaceships sent to all these planets have established whether they are habitable for humans or not, but sending a spaceship to each planet is expensive. In this problem, you will come up with decision trees to predict if a planet is habitable\n",
    "based only on features observable using telescopes.\n",
    "\n",
    "In the following table you are given the data from all 800 planets surveyed so far. The features observed by telescope are Size (“Big” or “Small”), and Orbit (“Near” or “Far”). Each row indicates the values of the features and habitability, and how many times that set of values was observed. So, for example, there were 20 “Big” planets “Near” their star that\n",
    "were habitable.\n",
    "\n",
    "| <center>Size (Big / Small)</center> | <center>Orbit (Near / Far)</center> | <center>Habitable (Yes / No)</center> | <center> Count</center>\n",
    "| --- | --- | --- | --- |\n",
    "|<center> Big </center>| <center> Near </center>| <center>Yes</center>| <center> 20 </center>\n",
    "|<center> Big | <center> Far </center>| <center>Yes</center>| <center> 170 </center>\n",
    "|<center> Small | <center> Near </center>| <center>Yes</center>| <center> 139 </center>\n",
    "|<center> Small | <center> Far </center>| <center>Yes</center>| <center> 45 </center>\n",
    "|<center> Big | <center> Near </center>| <center>No</center>| <center> 130 </center>\n",
    "|<center> Big | <center> Far </center>| <center>No</center>| <center> 30 </center>\n",
    "|<center> Small | <center> Near </center>| <center>No</center>| <center> 11 </center>\n",
    "| <center>Small | <center> Far </center>| <center>No</center>| <center> 255 </center>\n",
    "| 190+, 160- / 184+, 266- | <center> 159+, 141- / 215+, 285- </center>| <center> 374 / 426 </center>| <center> 800 </center>\n",
    "\n",
    "Derive and draw the decision tree on this data (use the maximum information gain (entropy) criterion for splits, don’t do any pruning).\n",
    "#### <img src=\"https://img.icons8.com/dusk/64/000000/idea.png\" style=\"height:30px;display:inline\"> Solution 1\n",
    "Let's first calculate the entropy of starting state: $$ H(Habitable) = -\\frac{20 + 170 + 139+45}{800} \\log_2 \\frac{20 + 170 + 139+45}{800} -\\frac{130 + 30 + 11+255}{800} \\log_2 \\frac{130 + 30 + 11+255}{800} = -\\frac{374}{800} \\log_2 \\frac{374}{800} -\\frac{426}{800} \\log_2 \\frac{426}{800} = 0.4841 + 0.5183 \\approx 1 $$\n",
    "\n",
    "Let's calculate the **Information Gain** from each split:\n",
    "$$ H(Habitable|Size) = \\frac{190+160}{800}H(Size=Big) + \\frac{184+266}{800}H(Size=Small)  = \\frac{350}{800} \\cdot (-\\frac{190}{350}\\log_2 (\\frac{190}{350}) -\\frac{160}{350}\\log_2 (\\frac{160}{350})) + \\frac{450}{800}\\cdot(-\\frac{184}{450}\\log_2 (\\frac{184}{450}) -\\frac{266}{450}\\log_2 (\\frac{266}{450})) = 0.984 \\rightarrow IG(Habitable|Size) = 1 - 0.984 = 0.016 $$\n",
    "\n",
    "$$ H(Habitable|Orbit) = \\frac{159+141}{800}H(Orbit=Near) + \\frac{215 + 285}{800}H(Orbit=Far)  = \\frac{300}{800} \\cdot (-\\frac{159}{300}\\log_2 (\\frac{159}{300}) -\\frac{141}{300}\\log_2 (\\frac{141}{300})) + \\frac{500}{800}\\cdot(-\\frac{215}{500}\\log_2 (\\frac{215}{500}) -\\frac{285}{500}\\log_2 (\\frac{282}{500})) = 0.9901 \\rightarrow IG(Habitable|Orbit) = 1 - 0.9901 = 0.009 $$\n",
    "\n",
    "So the Information Gain from the size is larger, thus, the first split will be based on the Size feature.\n",
    "<img src='./assets/tut_07_example_1.jpg' style=\"height:200px\">\n",
    "\n",
    "The final tree looks like this:\n",
    "<img src='./assets/tut_07_example_2.jpg' style=\"height:200px\">\n",
    "\n",
    "#### <a href=\"https://profs.info.uaic.ro/~ciortuz/ML.ex-book/SLIDES/ML.ex-book.SLIDES.DT.pdf\"> More Decision Trees Practice Excercises </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/dusk/50/000000/system-report.png\" style=\"height:50px;display:inline\"> Computational Complexity\n",
    "This section's material goes beyond the scope of this course, but it is important to understand why we can't really build an **optimal** tree and what is the computational cost of training and making predicitons using the built tree.\n",
    "* Denote $m$ as the number of instances (the training set size) and $n$ as the number of features.\n",
    "* Finding the **optimal tree** is known to be an *NP-Complete* problem, as it requires $O(exp(m))$ time, making the problem intractable even for small training sets.\n",
    "    * *P* is the set of problems that can be solved in polynomial time. *NP* is the set of problems whose solutions can be verfied in polynomial time. An *NP-Hard* problem is a problem which any *NP* problem can be reduced in polynimial time. An *NP-Complete* is both *NP* and *NP-Hard*.\n",
    "* **Making predicitons** requires traversing the Decision Tree from root to a leaf. Decision Trees are generally approximately balanced, so traversing a Decision Tree requires going through roughly $O(\\log_2(m))$ nodes. Since each node only requires checking the value of one feature, the overall prediciton complexity is just $O(\\log_2(m))$, independent of the number of features (so prediciton is quite **fast**, even with large training sets).\n",
    "* **Training or building the tree** requires comparing all features on all samples at each node. This results in a training complexity of $O(n \\times m \\log (m))$. For small training sets (less than a few thousands instances), Scikit-Learn can speed up training by presorting the data (set `presort=True`), but this slows down training considerably for larger training sets.\n",
    "\n",
    "### <img src=\"https://img.icons8.com/doodle/96/000000/tree.png\" style=\"height:50px;display:inline\"> Highly-Branchig Attributes\n",
    "* **Problem**: attributes with a large number of values\n",
    "    * Extreme case: each example has its own value. E.g. example ID, Date/Day attribute in weather data\n",
    "* Information gain is **biased** towards choosing attributes with a large number of values\n",
    "* This may cause:\n",
    "    * **Overfitting** - selection of an attribute that is non-optimal for prediction\n",
    "    * **Fragmentation** - data is fragmented into (too) many small sets\n",
    "    \n",
    "### <img src=\"https://img.icons8.com/plasticine/100/000000/idea.png\" style=\"height:30px;display:inline\"> Possible Solution - Normalization of the Inforamtion Gain of Features\n",
    "* Define **Split Information** of feature $A$ as the intrinsic information of a split:\n",
    "    * Entropy of distribution of instances into branches, or, how much information do we need to tell which branch an instance belongs to.\n",
    "    * Formally: $$\\textit{SplitInformation}(S,A) = -\\sum_{v \\in \\textit{Values}(A)} p_vlog(p_v) $$\n",
    "    * Observation: attributes with higher intrinsic information are less useful\n",
    "    * Another way of defining it is $\\textit{SplitInformation}(S,A) = \\log n(A)$, where $n(A)$ is the number of different values feature $A$ can split over the group of samples $S$\n",
    "* **Gain Ratio**\n",
    "    * Modification of the information gain (IG) that reduces its bias towards multi-valued atributes.\n",
    "    * Takes number of and size of branches into account when chossing an attribute\n",
    "        * Corrects the IG by taking the intrinsic information of a split into account\n",
    "    * Formally: $$ GR(S,A) =  \\frac{IG(S,A)}{\\textit{SplitInformation}(S,A)}$$\n",
    "    \n",
    "## <img src=\"https://img.icons8.com/cotton/64/000000/roadblock.png\" style=\"height:50px;display:inline\"> Prunning & Regularization Hyperparameters\n",
    "* **Underfitting & Overfitting** - if we allow Decision Trees to grow until *absolute purity*, we will reach overfitting\n",
    "    * <img src='./assets/tut_07_overfitting.jpg' style=\"height:200px\">\n",
    "    * Such model is often called a *nonparametric model*, not because it does not have many parameters (it often has a lot!), but because the number of parameters is not determined prior to learning (the tree keeps on growing).\n",
    "* Decision Trees make very few assumptions about the training data (unlike linear models, which assume that the data is linear).\n",
    "* To avoid **overfitting** the training data, we need to restrict the Decision Tree's freedom during training - this is called **regularization**.\n",
    "* The methods:\n",
    "    * **Stop Splitting** - stop growing a tree once a stopping criteria is reached\n",
    "        * May suffer from the *horizon effect* - such a method will be biased towards trees in which greatest impurity reduction is near the root\n",
    "    * **Prunning** - grow a full tree, then merge nodes until a stopping criteria is reached or deleting (=prunning) unneccessary nodes if the purity improvement it provides is not *statistically signifacnt*\n",
    "        * Use Hypothesis Testing - going back to Tutorial 02, standard statistical tests, such as chi-square,  $\\chi^2 \\textit{test}$, are used to estimate the probability (*p-value*) that the improvement is purely the result of chance (the *null hypothesis*, $H_0$). If the *p-value* is higher than a given threshold (usually 5%), then the node is considered unneccessary and its children are deleted). The prunning continues until all unnccessary nodes have been pruned.\n",
    "* **Common Stopping Criteria** (in practice):\n",
    "    * <a style=\"color:red\">*Minimum number of samples*</a> in leaf nodes - **increasing** will regularize the model\n",
    "        * In Scikit-Learn, set the input parameter `min_samples_leaf`\n",
    "    * <a style=\"color:red\">*Minimum samples split*</a> - the minimum number of samples a node must have before it can split, **increasing** will regularize the model\n",
    "        * In Scikit-Learn, set the input parameter `min_samples_split`\n",
    "    * <a style=\"color:red\">*Limit the depth of the tree*</a> - **reducing** the max depth will regularize the model (less chance of overfitting)\n",
    "       * In Scikit-Learn, set the input parameter `max_depth` (the default is `None` which means unlimited)\n",
    "    * <a style=\"color:red\">*Maximum number of leaves*</a> - maximum number of leaf nodes, **reducing** will regularize the model\n",
    "       * In Scikit-Learn, set the input parameter `max_leaf_nodes`\n",
    "    * Using the <a style=\"color:red\">validation accuracy</a>, stop if no significant improvement\n",
    "    * Also consider <a style=\"color:red\">`min_weight_fraction_leaf`</a> and <a style=\"color:red\">`max_features`</a> (more in the Scikit-Learn docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <img src=\"https://img.icons8.com/color/96/000000/cruise-ship.png\" style=\"height:50px;display:inline\"> Example - Titanic Dataset\n",
    "The sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\n",
    "\n",
    "One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n",
    "\n",
    "We will build a decision tree to decide what sorts of people were likely to survive.\n",
    "\n",
    "To read more about the dataset and the fields - <a href=\"https://www.kaggle.com/c/titanic/data\">Click Here</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>83</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>McDermott, Miss. Brigdet Delia</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330932</td>\n",
       "      <td>7.7875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>284</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Dorking, Mr. Edward Arthur</td>\n",
       "      <td>male</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5. 10482</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>586</th>\n",
       "      <td>587</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Jarvis, Mr. John Denzil</td>\n",
       "      <td>male</td>\n",
       "      <td>47.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>237565</td>\n",
       "      <td>15.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569</th>\n",
       "      <td>570</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Jonsson, Mr. Carl</td>\n",
       "      <td>male</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>350417</td>\n",
       "      <td>7.8542</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>192</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Carbines, Mr. William</td>\n",
       "      <td>male</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28424</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>845</th>\n",
       "      <td>846</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Abbing, Mr. Anthony</td>\n",
       "      <td>male</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>C.A. 5547</td>\n",
       "      <td>7.5500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>558</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Robbins, Mr. Victor</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17757</td>\n",
       "      <td>227.5250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>119</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Baxter, Mr. Quigg Edmond</td>\n",
       "      <td>male</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>PC 17558</td>\n",
       "      <td>247.5208</td>\n",
       "      <td>B58 B60</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>185</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Kink-Heilmann, Miss. Luise Gretchen</td>\n",
       "      <td>female</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>315153</td>\n",
       "      <td>22.0250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>66</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Moubarek, Master. Gerios</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2661</td>\n",
       "      <td>15.2458</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived  Pclass                                 Name  \\\n",
       "82            83         1       3       McDermott, Miss. Brigdet Delia   \n",
       "283          284         1       3           Dorking, Mr. Edward Arthur   \n",
       "586          587         0       2              Jarvis, Mr. John Denzil   \n",
       "569          570         1       3                    Jonsson, Mr. Carl   \n",
       "191          192         0       2                Carbines, Mr. William   \n",
       "845          846         0       3                  Abbing, Mr. Anthony   \n",
       "557          558         0       1                  Robbins, Mr. Victor   \n",
       "118          119         0       1             Baxter, Mr. Quigg Edmond   \n",
       "184          185         1       3  Kink-Heilmann, Miss. Luise Gretchen   \n",
       "65            66         1       3             Moubarek, Master. Gerios   \n",
       "\n",
       "        Sex   Age  SibSp  Parch      Ticket      Fare    Cabin Embarked  \n",
       "82   female   NaN      0      0      330932    7.7875      NaN        Q  \n",
       "283    male  19.0      0      0  A/5. 10482    8.0500      NaN        S  \n",
       "586    male  47.0      0      0      237565   15.0000      NaN        S  \n",
       "569    male  32.0      0      0      350417    7.8542      NaN        S  \n",
       "191    male  19.0      0      0       28424   13.0000      NaN        S  \n",
       "845    male  42.0      0      0   C.A. 5547    7.5500      NaN        S  \n",
       "557    male   NaN      0      0    PC 17757  227.5250      NaN        C  \n",
       "118    male  24.0      0      1    PC 17558  247.5208  B58 B60        C  \n",
       "184  female   4.0      0      2      315153   22.0250      NaN        S  \n",
       "65     male   NaN      1      1        2661   15.2458      NaN        C  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's load the titanic dataset and speratre it into train and test set\n",
    "dataset = pd.read_csv('./datasets/titanic_dataset.csv')\n",
    "# print the number of rows in the data set\n",
    "number_of_rows = len(dataset)\n",
    "num_train = int(0.8 * number_of_rows)\n",
    "# reminder, the data looks like this\n",
    "dataset.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Has_Cabin</th>\n",
       "      <th>FamilySize</th>\n",
       "      <th>IsAlone</th>\n",
       "      <th>Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>526</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>747</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Survived  Pclass  Sex  Age  Parch  Fare  Embarked  Has_Cabin  FamilySize  \\\n",
       "55          1       1    1    2      0     3         0          1           1   \n",
       "318         1       1    0    1      2     3         0          1           3   \n",
       "456         0       1    1   65      0     2         0          1           1   \n",
       "526         1       2    0    3      0     1         0          0           1   \n",
       "747         1       2    0    1      0     1         0          0           1   \n",
       "453         1       1    1    3      0     3         1          1           2   \n",
       "184         1       3    0    0      2     2         0          0           3   \n",
       "355         0       3    1    1      0     1         0          0           1   \n",
       "316         1       2    0    1      0     2         0          0           2   \n",
       "276         0       3    0    2      0     0         0          0           1   \n",
       "\n",
       "     IsAlone  Title  \n",
       "55         1      1  \n",
       "318        0      4  \n",
       "456        1      1  \n",
       "526        1      4  \n",
       "747        1      4  \n",
       "453        0      1  \n",
       "184        0      4  \n",
       "355        1      1  \n",
       "316        0      3  \n",
       "276        1      4  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preprocessing\n",
    "original_ds = dataset.copy() # Using 'copy()' allows to clone the dataset, creating a different object with the same values\n",
    "dataset['Has_Cabin'] = dataset[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\n",
    "# Create new feature FamilySize as a combination of SibSp and Parch\n",
    "dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n",
    "# Create new feature IsAlone from FamilySize\n",
    "dataset['IsAlone'] = 0\n",
    "dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n",
    "# Remove all NULLS in the Embarked column\n",
    "dataset['Embarked'] = dataset['Embarked'].fillna('S')\n",
    "# Remove all NULLS in the Fare column\n",
    "dataset['Fare'] = dataset['Fare'].fillna(dataset['Fare'].median())\n",
    "\n",
    "# Remove all NULLS in the Age column\n",
    "age_avg = dataset['Age'].mean()\n",
    "age_std = dataset['Age'].std()\n",
    "age_null_count = dataset['Age'].isnull().sum()\n",
    "age_null_random_list = np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_count)\n",
    "# Next line has been improved to avoid warning\n",
    "dataset.loc[np.isnan(dataset['Age']), 'Age'] = age_null_random_list\n",
    "dataset['Age'] = dataset['Age'].astype(int)\n",
    "\n",
    "# Define function to extract titles from passenger names\n",
    "def get_title(name):\n",
    "    title_search = re.search(' ([A-Za-z]+)\\.', name)\n",
    "    # If the title exists, extract and return it.\n",
    "    if title_search:\n",
    "        return title_search.group(1)\n",
    "    return \"\"\n",
    "\n",
    "dataset['Title'] = dataset['Name'].apply(get_title)\n",
    "\n",
    "# Group all non-common titles into one single grouping \"Rare\"\n",
    "dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major',\n",
    "                                             'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
    "dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n",
    "dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n",
    "dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n",
    "\n",
    "# mapping\n",
    "# Mapping Sex\n",
    "dataset['Sex'] = dataset['Sex'].map( {'female': 0, 'male': 1} ).astype(int)    \n",
    "# Mapping titles\n",
    "title_mapping = {\"Mr\": 1, \"Master\": 2, \"Mrs\": 3, \"Miss\": 4, \"Rare\": 5}\n",
    "dataset['Title'] = dataset['Title'].map(title_mapping)\n",
    "dataset['Title'] = dataset['Title'].fillna(0)\n",
    "# Mapping Embarked\n",
    "dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n",
    "# Mapping Fare\n",
    "dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n",
    "dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n",
    "dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']  = 2\n",
    "dataset.loc[ dataset['Fare'] > 31, 'Fare'] = 3\n",
    "dataset['Fare'] = dataset['Fare'].astype(int)\n",
    "\n",
    "# Mapping Age\n",
    "dataset.loc[ dataset['Age'] <= 16, 'Age'] = 0\n",
    "dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n",
    "dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n",
    "dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n",
    "dataset.loc[ dataset['Age'] > 64, 'Age']\n",
    "\n",
    "# Feature selection: remove variables no longer containing relevant information\n",
    "drop_elements = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'SibSp']\n",
    "dataset = dataset.drop(drop_elements, axis=1)\n",
    "\n",
    "# show a sample of the preprocessed dataset\n",
    "dataset.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total training samples: 712, total test samples: 179\n"
     ]
    }
   ],
   "source": [
    "# Take the relevant training data and labels\n",
    "x = dataset.drop('Survived', axis=1).values\n",
    "y = dataset['Survived'].values  # 1 for Survived, 0 for Dead\n",
    "# shuffle\n",
    "rand_gen = np.random.RandomState(0)\n",
    "shuffled_indices = rand_gen.permutation(np.arange(len(x)))\n",
    "\n",
    "x_train = x[shuffled_indices[:num_train]]\n",
    "y_train = y[shuffled_indices[:num_train]]\n",
    "x_test = x[shuffled_indices[num_train:]]\n",
    "y_test = y[shuffled_indices[num_train:]]\n",
    "\n",
    "print(\"total training samples: {}, total test samples: {}\".format(num_train, number_of_rows - num_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's build a tree with no limitations\n",
    "tree_clf = DecisionTreeClassifier(criterion='gini')\n",
    "tree_clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 73.743 %\n"
     ]
    }
   ],
   "source": [
    "# let's check the accuracy on the test set\n",
    "y_pred = tree_clf.predict(x_test)\n",
    "accuracy = np.sum(y_pred == y_test) / len(y_test)\n",
    "print(\"accuracy: {:.3f} %\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 84.358 %\n"
     ]
    }
   ],
   "source": [
    "# let's add regularization\n",
    "tree_clf = DecisionTreeClassifier(criterion='gini', max_depth=3)\n",
    "tree_clf.fit(x_train, y_train)\n",
    "# let's check the accuracy on the test set\n",
    "y_pred = tree_clf.predict(x_test)\n",
    "accuracy = np.sum(y_pred == y_test) / len(y_test)\n",
    "print(\"accuracy: {:.3f} %\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passenger 0 chances of survival: 89.254 %\n"
     ]
    }
   ],
   "source": [
    "# predicting probabilities\n",
    "# for the first passenger in the test set, the probability of survival:\n",
    "prob = tree_clf.predict_proba([x_train[0]])\n",
    "print(\"passenger 0 chances of survival: {:.3f} %\".format(prob[0][0] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate an image graph\n",
    "from sklearn.tree import export_graphviz\n",
    "export_graphviz(tree_clf, out_file=\"./titanic_tree.dot\", feature_names=dataset.columns.values[1:].tolist(), \n",
    "               class_names=['Dead', 'Survived'], rounded=True, filled=True)\n",
    "# open the file with your favourite text editor and copy-pase it into http://www.webgraphviz.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./assets/tut_07_titan_tree.jpg\" style=\"height:400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <img src=\"https://img.icons8.com/dusk/64/000000/prize.png\" style=\"height:50px;display:inline\"> Credits\n",
    "* Icons from <a href=\"https://icons8.com/\">Icon8.com</a> - https://icons8.com\n",
    "* Datasets from <a href=\"https://www.kaggle.com/\">Kaggle</a> - https://www.kaggle.com/\n",
    "* Examples and code snippets were taken from <a href=\"http://shop.oreilly.com/product/0636920052289.do\">\"Hands-On Machine Learning with Scikit-Learn and TensorFlow\"</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
